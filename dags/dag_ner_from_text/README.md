# ğŸ·ï¸ Named Entity Recognition (NER) DAG

> **DAG ID:** `com.nlp.ner_from_text`  
> **Model:** PhoBERT-base  
> **Language:** Vietnamese  
> **Version:** 1.0.0

---

## ğŸ“– Giá»›i thiá»‡u

DAG nÃ y dÃ¹ng Ä‘á»ƒ **huáº¥n luyá»‡n mÃ´ hÃ¬nh nháº­n dáº¡ng thá»±c thá»ƒ Ä‘Æ°á»£c Ä‘áº·t tÃªn (NER)** cho tiáº¿ng Viá»‡t báº±ng **PhoBERT** - má»™t mÃ´ hÃ¬nh BERT Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c dÃ nh riÃªng cho ngÃ´n ngá»¯ Viá»‡t.

### á»¨ng dá»¥ng

- ğŸ¢ TrÃ­ch xuáº¥t tÃªn cÃ´ng ty, tá»• chá»©c
- ğŸ‘¤ XÃ¡c Ä‘á»‹nh tÃªn ngÆ°á»i
- ğŸ“ Äá»‹nh vá»‹ Ä‘á»‹a danh
- ğŸ“… Nháº­n dáº¡ng thá»i gian, ngÃ y thÃ¡ng
- ğŸ“„ TrÃ­ch xuáº¥t thÃ´ng tin tá»« tÃ i liá»‡u

### CÃ¡c loáº¡i thá»±c thá»ƒ há»— trá»£

| Loáº¡i | KÃ½ hiá»‡u | VÃ­ dá»¥ |
|------|---------|-------|
| **Person** | PER | Nguyá»…n VÄƒn A, Há»“ ChÃ­ Minh |
| **Organization** | ORG | CÃ´ng ty Google, Bá»™ GiÃ¡o dá»¥c |
| **Location** | LOC | HÃ  Ná»™i, Má»¹, SÃ´ng Há»“ng |
| **Date/Time** | DATE | NgÃ y 1/1/2024, ThÃ¡ng 3 |
| **Miscellaneous** | MISC | Tiáº¿ng Anh, tÃ´n giÃ¡o Pháº­t |

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u

Táº¡o cÃ¡c file theo Ä‘á»‹nh dáº¡ng **CoNLL** (má»™t token + nhÃ£n trÃªn má»—i dÃ²ng):

```
dags/data/ner_text_input/
â”œâ”€â”€ train.txt
â”œâ”€â”€ dev.txt
â””â”€â”€ test.txt
```

**Äá»‹nh dáº¡ng file (CoNLL):**
```
Nguyá»…n O
VÄƒn B-PER
A I-PER
lÃ m O
viá»‡c O
táº¡i O
CÃ´ng B-ORG
ty I-ORG
Google I-ORG
. O

(empty line = sentence separator)
```

### BÆ°á»›c 2: Trigger DAG

```
Airflow UI â†’ DAG "com.nlp.ner_from_text" â†’ Trigger DAG
```

### BÆ°á»›c 3: Láº¥y káº¿t quáº£

```
dags/data/ner_text_output/
â”œâ”€â”€ model_export/
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ label_config.json
â””â”€â”€ predictions/
    â””â”€â”€ test_predictions.txt
```

---

## ğŸ”„ Pipeline

```
prepare_dataset >> train_ner_model >> evaluate_ner_model >> export_ner_model
```

| Task | Chá»©c nÄƒng | Input | Output |
|------|-----------|-------|--------|
| **prepare_dataset** | Táº£i dá»¯ liá»‡u CoNLL | train.txt, dev.txt, test.txt | Tokenized dataset |
| **train_ner_model** | Huáº¥n luyá»‡n PhoBERT | Dataset | Trained model |
| **evaluate_ner_model** | ÄÃ¡nh giÃ¡ trÃªn test set | Trained model | Metrics (F1, Precision, Recall) |
| **export_ner_model** | Export Ä‘á»ƒ inference | Trained model | Model files + label mapping |

---

## ğŸ§  PhoBERT Model

### ThÃ´ng sá»‘ ká»¹ thuáº­t

| ThÃ´ng sá»‘ | GiÃ¡ trá»‹ |
|----------|--------|
| **Model** | vinai/phobert-base |
| **Type** | RoBERTa-based |
| **Parameters** | ~135M |
| **Vocab** | 64K tokens |
| **Max length** | 256 tokens |
| **Pretrained on** | Vietnamese Wikipedia + News |

### TÃ­nh nÄƒng

- âœ… **Pretrained trÃªn tiáº¿ng Viá»‡t**: Tá»‘i Æ°u cho ngÃ´n ngá»¯ Viá»‡t
- âœ… **Word segmentation**: Tá»± Ä‘á»™ng cáº¯t tá»«
- âœ… **Transformer architecture**: Hiá»‡u quáº£ cao
- âœ… **Fine-tuning nhanh**: Há»™i tá»¥ nhanh trÃªn data nhá»

---

## âš™ï¸ Cáº¥u hÃ¬nh Training

Chá»‰nh sá»­a trong `config.py`:

```python
NER_CONFIG = {
    'model_name': 'vinai/phobert-base',
    'max_seq_length': 256,      # Äá»™ dÃ i tá»‘i Ä‘a
    'batch_size': 32,           # KÃ­ch thÆ°á»›c batch
    'num_epochs': 10,           # Sá»‘ epoch
    'learning_rate': 5e-5,      # Learning rate
    'warmup_steps': 500,        # Warmup steps
}
```

### CÃ¡c thÃ´ng sá»‘ quan trá»ng

- **max_seq_length**: TÄƒng náº¿u cÃ¢u dÃ i, giáº£m Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›
- **batch_size**: TÄƒng Ä‘á»ƒ training nhanh hÆ¡n (náº¿u GPU Ä‘á»§)
- **num_epochs**: 10-20 epochs thÆ°á»ng Ä‘á»§
- **learning_rate**: 5e-5 hoáº·c 2e-5 cho fine-tuning

---

## ğŸ“Š Äá»‹nh dáº¡ng Dá»¯ liá»‡u

### Input Format (CoNLL)

Má»—i dÃ²ng = 1 token + 1 nhÃ£n (cÃ¡ch báº±ng khoáº£ng tráº¯ng hoáº·c tab):

```
word1 label1
word2 label2
...
(empty line for sentence boundary)
```

**VÃ­ dá»¥:**
```
CÃ´ng B-ORG
ty I-ORG
Google I-ORG
tuyá»ƒn O
dá»¥ng O
nhÃ¢n O
viÃªn O
táº¡i O
HÃ  B-LOC
Ná»™i I-LOC
. O

(empty line here)

Há» B-PER
cÃ³ O
má»©c O
lÆ°Æ¡ng O
cao O
. O
```

### Label Tags

```
B-LABEL  = Beginning of entity
I-LABEL  = Inside/continuation of entity
O        = Outside any entity
```

### VÃ­ dá»¥ vá»›i NER tags

```
Nguyá»…n B-PER
VÄƒn I-PER
A I-PER
lÃ m O
viá»‡c O
táº¡i O
Google B-ORG
. O
```

---

## ğŸ“ Output Structure

```
dags/data/ner_text_output/
â”œâ”€â”€ model_export/
â”‚   â”œâ”€â”€ pytorch_model.bin          # Model weights
â”‚   â”œâ”€â”€ config.json                # Model config
â”‚   â”œâ”€â”€ tokenizer.json             # Tokenizer
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â””â”€â”€ label_config.json          # Label mapping
â”‚
â””â”€â”€ predictions/
    â”œâ”€â”€ test_predictions.txt       # Raw predictions
    â””â”€â”€ metrics.json               # F1, Precision, Recall
```

---

## ğŸ› ï¸ YÃªu cáº§u Dependencies

```bash
pip install transformers>=4.30.0
pip install datasets>=2.10.0
pip install torch>=2.0.0
pip install seqeval              # For NER metrics
pip install fire                 # CLI
```

ThÃªm vÃ o requirements.txt:

```
transformers>=4.30.0
datasets>=2.10.0
seqeval>=2.2.1
```

---

## ğŸ“Š Metrics

DAG sá»­ dá»¥ng cÃ¡c metrics NER tiÃªu chuáº©n:

### Token-level metrics
- **Accuracy**: Tá»· lá»‡ token Ä‘Æ°á»£c phÃ¢n loáº¡i Ä‘Ãºng

### Entity-level metrics (seqeval)
- **Precision**: P = correct / predicted
- **Recall**: R = correct / gold
- **F1-score**: Harmonic mean cá»§a precision vÃ  recall

---

## ğŸ’¡ Tips

1. **CÃ¢n báº±ng dá»¯ liá»‡u**: NÃªn cÃ³ sá»‘ lÆ°á»£ng entity tÆ°Æ¡ng Ä‘Æ°Æ¡ng giá»¯a cÃ¡c loáº¡i
2. **Data cleaning**: XÃ³a duplicate, fix encoding trÆ°á»›c huáº¥n luyá»‡n
3. **Validation set**: DÃ¹ng dev.txt Ä‘á»ƒ monitor overfitting
4. **Checkpoint**: Model tá»± save best checkpoint theo F1 score
5. **GPU**: NÃªn dÃ¹ng GPU (5-10x nhanh hÆ¡n CPU)

---

## ğŸ“ LÆ°u Ã½

- PhoBERT khÃ´ng tá»± Ä‘á»™ng cáº¯t tá»«, cáº§n word-segmented input
- Labels pháº£i chá»©a "O" tag á»Ÿ Ä‘áº§u
- Token Ä‘Æ°á»£c pad vá»›i label `-100` (ignored in loss calculation)
- Model Ä‘Æ°á»£c save dÆ°á»›i Ä‘á»‹nh dáº¡ng PyTorch

---

## ğŸ”— LiÃªn káº¿t

- [PhoBERT GitHub](https://github.com/VinAIResearch/PhoBERT)
- [Phobert NER Reference](https://github.com/Avi197/Phobert-Named-Entity-Reconigtion)
- [HuggingFace Transformers](https://huggingface.co/transformers/)
- [seqeval Metrics](https://github.com/chakki-works/seqeval)
- [CoNLL Format](https://www.clips.uantwerpen.be/conll2003/ner/)
